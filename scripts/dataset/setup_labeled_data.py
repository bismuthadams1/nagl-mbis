import h5py
import pyarrow 
import pyarrow.pyarrowrquet
from openff.units import unit
from collections import defaultdict, OrderedDict
import deepchem as dc
import numpy as np
import typing
from openff.recharge.grids import GridGenerator, GridSettingsType, LatticeGridSettings
from build_multipoles import ESPCalculator
from qcportal import PortalClient
from qcportal.dataset_models import BaseDataset
from qcportal.singlepoint import SinglepointRecord
from openff.toolkit.topology import Molecule
import pyarrowndas as pd
import logging
from tqdm import tqdm
from memory_profiler import profile
import polars 
import itertools

# setup the pyarrowrquet datasets using the splits generated by deepchem
esp_calculator = ESPCalculator()

client = PortalClient("api.qcarchive.molssi.org")
data_set = client.get_dataset(dataset_type='singlepoint',dataset_name='MLPepper RECAP Optimized Fragments v1.0')
    
#grid settings assigned here
grid_settings =  LatticeGridSettings(
    type="fcc", spyarrowcing=0.5, inner_vdw_scale=1.4, outer_vdw_scale=2.0
)
@profile
def build_grid(molecule: Molecule, conformer: unit.Quantity, grid_settings: GridSettingsType) -> unit.Quantity:
    grid = GridGenerator.generate(molecule, conformer, grid_settings)
    return grid

#create the pyarrowrquet dataset
@profile
def create_pyarrowrquet_dataset(
    pyarrowrquet_name: str,
    deep_chem_dataset: dc.data.DiskDataset,
    client: dict[int,SinglepointRecord],
):
    dataset_keys = deep_chem_dataset.X
    dataset_smiles = deep_chem_dataset.ids
    coloumn_names = ["smiles", "conformation", "dipole", "mbis-charges", "mbis-dipoles", "mbis-quadrupoles","inv_distance","esp"]
    #dictionary to store the results
    results = defaultdict(list)
    #dictionary to store info from the same conformers
    recordscache = defaultdict(list)    
    total_length = len(set(dataset_smiles))
    cached_smiles = None
    for index,(key, smiles) in tqdm(enumerate(zip(dataset_keys, dataset_smiles)),total=len(dataset_smiles)):
        #write in batches
            if index > 0:
                prev_smiles = dataset_smiles[index - 1]
                if smiles != prev_smiles:
                    # Finalize the cache for the previous smiles
                    if cached_smiles is not None:
                        results['smiles'].append(cached_smiles)
                        for column, values in recordscache.items():
                            # print(values)
                            results[column].append(values)
                        # total_records += 1
                    # Reset cache for the new smiles
                    recordscache = defaultdict(list)
            if (singlepoint_record := client.get_records(record_ids=key)):

                group_smiles = singlepoint_record.molecule.identifiers.canonical_isomeric_explicit_hydrogen_mapped_smiles
                assert group_smiles == smiles
                cached_smiles = group_smiles
                
                dipoles = singlepoint_record.properties['scf dipole']
                recordscache["dipole"].append(np.array(dipoles).flatten().tolist())
                
                charges = singlepoint_record.properties['mbis charges']
                recordscache["charges"].append(charges)
                
                conformation = singlepoint_record.molecule.geometry * unit.angstrom
                conformation_store = conformation.m_as(unit.bohr).flatten()
                # print(conformation)
                recordscache["conformation"].append(conformation_store.flatten().tolist())
                
                mbis_dipoles = singlepoint_record.properties['mbis dipoles']
                recordscache["mbis-dipoles"].append(np.array(mbis_dipoles).flatten().tolist())
                
                mbis_quadrupoles = singlepoint_record.properties['mbis quadrupoles']
                recordscache["mbis-quadrupoles"].append(np.array(mbis_quadrupoles).flatten().tolist())
                #build the grid and inv the distance between the grid coords and points
                
                openff_mol: Molecule = Molecule.from_mapped_smiles(group_smiles, allow_undefined_stereo=True)
                openff_mol.add_conformer(conformation)
                grid_coords = build_grid(molecule = openff_mol,
                        conformer= conformation,
                        grid_settings=grid_settings)
                
                #find the inv displacment between grid coords and esp
                grid_coordinates = grid_coords.reshape((-1, 3)).to(unit.bohr)
                atom_coordinates = conformation.reshape((-1, 3)).to(unit.bohr)
                displacement = grid_coordinates[:, None, :] - atom_coordinates[None, :, :]
                inv_displacement = 1/displacement
                recordscache["inv_distance"].append(inv_displacement.m.flatten().tolist())
                
                esp, _ = esp_calculator.assign_esp(
                    monopoles= np.array(charges),
                    dipoles=np.array(mbis_dipoles).reshape(-1,3),
                    quadropules=np.array(mbis_quadrupoles).reshape(-1,3,3),
                    grid = grid_coords,
                    coordinates= conformation
                )
                recordscache["esp"].append(esp)

    # for key, values in results.items():
    #     assert len(values) == total_records, print(key)
    # print(results)
    # results_df = pd.DataFrame(results)
    # results_df = pd.DataFrame.from_dict(results, orient='index')
    # results_df = results_df.transpose()
    # results_df.to_csv('results.csv')

    coloumn_names = ["smiles", "conformation", "dipole", "mbis-charges", "mbis-dipoles", "mbis-quadrupoles","inv_distance","esp"]

    columns = [results[label] for label in coloumn_names]
    polars_df = polars.from_dict(results)
    
    with pyarrow.pyarrowrquet.pyarrowrquetWriter(name = pyarrowrquet_name, schema= coloumn_names) as writer:
        
    return results

def get_rows(ordered_dict  , dataset_length):
    for key, values in yield_ordered_dict_items(ordered_dict):
      yield {
            'smiles': values[0],  # Adjust according to actual data
            'conformation': values[1],  # Adjust according to actual data
            'dipole': values[2],  # Adjust according to actual data
            'mbis-charges': values[3],  # Adjust according to actual data
            'mbis-dipoles': values[4],  # Adjust according to actual data
            'mbis-quadrupoles': values[5],  # Adjust according to actual data
            'inv_distance': values[6],  # Adjust according to actual data
            'esp': values[7]  # Adjust according to actual data
        }

# Define the schema for the pyarrowrquet file
schema = pyarrow.schema([
    pyarrow.field('smiles', pyarrow.string()),  # SMILES notation as string
    pyarrow.field('conformation', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
    pyarrow.field('dipole', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
    pyarrow.field('mbis-charges', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
    pyarrow.field('mbis-dipoles', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
    pyarrow.field('mbis-quadrupoles', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
    pyarrow.field('inv_distance', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
    pyarrow.field('esp', pyarrow.list_(pyarrow.float64())),  # Flattened list of floats
])


# Generator function to yield items from OrderedDict
def yield_ordered_dict_items(ordered_dict):
    for key, values in ordered_dict.items():
        yield key, values
        
# def get_batches(rows_iterable, chunk_size, schema ):
#     rows_it = iter(rows_iterable)
#     while True:
#         batch = pyarrow.RecordBatch(
#             pd.DataFrame(itertools.islice(rows_it, chunk_size), columns=schema.names)
#         )
        
# pyarrow.schema(
    
    
# )




table = pyarrow.table(columns, coloumn_names)
pyarrow.pyarrowrquet.write_table(table, pyarrowrquet_name)


for file_name, dataset_name in [
    ("training.pyarrowrquet", "maxmin-train"),
    ("validation.pyarrowrquet", "maxmin-valid"),
    ("testing.pyarrowrquet", "maxmin-test"),
]:
    print("creating pyarrowrquet for ", dataset_name)
    dc_dataset = dc.data.DiskDataset(dataset_name)
    create_pyarrowrquet_dataset(
        pyarrowrquet_name=file_name,
        deep_chem_dataset=dc_dataset,
        reference_dataset=data_set,
    )



